{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJal6oaClgoB",
        "outputId": "0b0294cb-c2ef-4d47-a95c-6a06f7ebb116"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix3Sgao3lmuN",
        "outputId": "32f29341-a932-4b6e-858d-9530a0603450"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.10.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import dill\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "from models_initialize import GCN\n",
        "from metrics import llprint, multi_label_metric, ddi_rate_score, get_n_params\n",
        "\n",
        "torch.manual_seed(1203)\n",
        "np.random.seed(1203)\n",
        "\n",
        "model_name = 'GCN'\n",
        "resume_name = ''\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--eval', action='store_true', default=False, help=\"eval mode\")\n",
        "parser.add_argument('--model_name', type=str, default=model_name, help=\"model name\")\n",
        "parser.add_argument('--resume_path', type=str, default=resume_name, help='resume path')\n",
        "parser.add_argument('--ddi', action='store_true', default=False, help=\"using ddi\")\n",
        "\n",
        "args, unknown = parser.parse_known_args()  # Replace here\n",
        "model_name = args.model_name\n",
        "resume_name = args.resume_path\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = '/content/patient_records_final.pkl'\n",
        "    voc_path = '/content/vocabulary_final.pkl'\n",
        "\n",
        "    ehr_adj_path = '/content/ehr_adjacency_matrix_final.pkl'\n",
        "    ddi_adj_path = '/content/ddi_adjacency_matrix_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n",
        "    ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "\n",
        "    # Define a set of hyperparameters to try out\n",
        "    emb_dim_values = [32, 64, 128]\n",
        "    dropout_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    lr_values = [0.0001, 0.0002, 0.0005]\n",
        "    epoch = 10\n",
        "    Neg_Loss = args.ddi\n",
        "    DDI_IN_MEM = args.ddi\n",
        "    best_score = 0\n",
        "    best_params = {}\n",
        "\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    # Try out all combinations of hyperparameters\n",
        "    for emb_dim in emb_dim_values:\n",
        "        for dropout in dropout_values:\n",
        "            for lr in lr_values:\n",
        "                print(f\"Trying parameters: emb_dim={emb_dim}, dropout={dropout}, LR={lr}, EPOCH={epoch}\")\n",
        "\n",
        "                # Create a new instance of the model with the current set of parameters\n",
        "                model = GCN(voc_size, ehr_adj, ddi_adj, emb_dim=emb_dim, dropout=dropout, device=torch.device('cpu:0'), ddi_in_memory=DDI_IN_MEM)\n",
        "                #model.to(device)\n",
        "\n",
        "                # Train the model and get its performance on the validation set\n",
        "                optimizer = Adam(model.parameters(), lr=lr)\n",
        "                criterion = CrossEntropyLoss()\n",
        "\n",
        "                model.train()\n",
        "                for current_epoch in range(epoch):\n",
        "                    loss_record1 = []\n",
        "                    prediction_loss_cnt = 0\n",
        "                    neg_loss_cnt = 0\n",
        "                    for step, input in enumerate(data_train):\n",
        "                      #input = input.to(device)\n",
        "                      for idx, adm in enumerate(input):\n",
        "                          seq_input = input[:idx+1]\n",
        "                          #seq_input = torch.tensor(seq_input).to(device)\n",
        "                          loss1_target = np.zeros((1, voc_size[2]))\n",
        "                          loss1_target[:, adm[2]] = 1\n",
        "                          loss3_target = np.full((1, voc_size[2]), -1)\n",
        "                          for idx, item in enumerate(adm[2]):\n",
        "                              loss3_target[0][idx] = item\n",
        "\n",
        "                          target_output1, batch_neg_loss = model(seq_input)\n",
        "\n",
        "                          loss1 = F.binary_cross_entropy_with_logits(target_output1, torch.FloatTensor(loss1_target).to(device))\n",
        "                          loss3 = F.multilabel_margin_loss(F.sigmoid(target_output1), torch.LongTensor(loss3_target).to(device))\n",
        "                          if Neg_Loss:\n",
        "                              target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
        "                              target_output1[target_output1 >= 0.5] = 1\n",
        "                              target_output1[target_output1 < 0.5] = 0\n",
        "                              y_label = np.where(target_output1 == 1)[0]\n",
        "                              current_ddi_rate = ddi_rate_score([[y_label]])\n",
        "                              if current_ddi_rate <= TARGET_DDI:\n",
        "                                  loss = 0.9 * loss1 + 0.01 * loss3\n",
        "                                  prediction_loss_cnt += 1\n",
        "                              else:\n",
        "                                  rnd = np.exp((TARGET_DDI - current_ddi_rate)/T)\n",
        "                                  if np.random.rand(1) < rnd:\n",
        "                                      loss = batch_neg_loss\n",
        "                                      neg_loss_cnt += 1\n",
        "                                  else:\n",
        "                                      loss = 0.9 * loss1 + 0.01 * loss3\n",
        "                                      prediction_loss_cnt += 1\n",
        "                          else:\n",
        "                              loss = 0.9 * loss1 + 0.01 * loss3\n",
        "\n",
        "                          optimizer.zero_grad()\n",
        "                          loss.backward(retain_graph=True)\n",
        "                          optimizer.step()\n",
        "\n",
        "                          loss_record1.append(loss.item())\n",
        "\n",
        "                llprint('\\rTrain--Epoch: %d, Step: %d/%d, L_p cnt: %d, L_neg cnt: %d' % (epoch, step, len(data_train), prediction_loss_cnt, neg_loss_cnt))\n",
        "\n",
        "                # Evaluate the model\n",
        "                ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "\n",
        "                # Choose a score to optimize\n",
        "                score = avg_f1\n",
        "\n",
        "                # If the score on the validation set is the best we have seen, save the parameters\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {\"emb_dim\": emb_dim, \"dropout\": dropout, \"LR\": lr, \"EPOCH\": epoch}\n",
        "\n",
        "    print(\"Best score: \", best_score)\n",
        "    print(\"Best parameters: \", best_params)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VlYH1REdXTo",
        "outputId": "8b44aaef-4e92-448f-e0f7-75b4467668ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying parameters: emb_dim=32, dropout=0.1, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0828, Jaccard: 0.4244,  PRAUC: 0.6569, AVG_PRC: 0.6375, AVG_RECALL: 0.5572, AVG_F1: 0.5661\n",
            "Trying parameters: emb_dim=32, dropout=0.1, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0909, Jaccard: 0.4297,  PRAUC: 0.6671, AVG_PRC: 0.6578, AVG_RECALL: 0.5522, AVG_F1: 0.5698\n",
            "Trying parameters: emb_dim=32, dropout=0.1, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1037, Jaccard: 0.4404,  PRAUC: 0.6809, AVG_PRC: 0.6438, AVG_RECALL: 0.5783, AVG_F1: 0.5787\n",
            "Trying parameters: emb_dim=32, dropout=0.2, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0822, Jaccard: 0.4316,  PRAUC: 0.6580, AVG_PRC: 0.6418, AVG_RECALL: 0.5618, AVG_F1: 0.5714\n",
            "Trying parameters: emb_dim=32, dropout=0.2, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0898, Jaccard: 0.4326,  PRAUC: 0.6694, AVG_PRC: 0.6469, AVG_RECALL: 0.5648, AVG_F1: 0.5727\n",
            "Trying parameters: emb_dim=32, dropout=0.2, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1020, Jaccard: 0.4474,  PRAUC: 0.6819, AVG_PRC: 0.6437, AVG_RECALL: 0.5927, AVG_F1: 0.5875\n",
            "Trying parameters: emb_dim=32, dropout=0.3, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0823, Jaccard: 0.4337,  PRAUC: 0.6603, AVG_PRC: 0.6437, AVG_RECALL: 0.5662, AVG_F1: 0.5740\n",
            "Trying parameters: emb_dim=32, dropout=0.3, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0899, Jaccard: 0.4323,  PRAUC: 0.6668, AVG_PRC: 0.6428, AVG_RECALL: 0.5688, AVG_F1: 0.5731\n",
            "Trying parameters: emb_dim=32, dropout=0.3, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0982, Jaccard: 0.4471,  PRAUC: 0.6829, AVG_PRC: 0.6455, AVG_RECALL: 0.5912, AVG_F1: 0.5867\n",
            "Trying parameters: emb_dim=32, dropout=0.4, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0807, Jaccard: 0.4294,  PRAUC: 0.6530, AVG_PRC: 0.6369, AVG_RECALL: 0.5607, AVG_F1: 0.5695\n",
            "Trying parameters: emb_dim=32, dropout=0.4, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0882, Jaccard: 0.4356,  PRAUC: 0.6754, AVG_PRC: 0.6574, AVG_RECALL: 0.5632, AVG_F1: 0.5773\n",
            "Trying parameters: emb_dim=32, dropout=0.4, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0976, Jaccard: 0.4486,  PRAUC: 0.6842, AVG_PRC: 0.6459, AVG_RECALL: 0.5906, AVG_F1: 0.5874\n",
            "Trying parameters: emb_dim=32, dropout=0.5, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0801, Jaccard: 0.4291,  PRAUC: 0.6546, AVG_PRC: 0.6397, AVG_RECALL: 0.5600, AVG_F1: 0.5700\n",
            "Trying parameters: emb_dim=32, dropout=0.5, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0879, Jaccard: 0.4325,  PRAUC: 0.6670, AVG_PRC: 0.6531, AVG_RECALL: 0.5597, AVG_F1: 0.5737\n",
            "Trying parameters: emb_dim=32, dropout=0.5, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0989, Jaccard: 0.4473,  PRAUC: 0.6818, AVG_PRC: 0.6476, AVG_RECALL: 0.5871, AVG_F1: 0.5861\n",
            "Trying parameters: emb_dim=64, dropout=0.1, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0896, Jaccard: 0.4314,  PRAUC: 0.6670, AVG_PRC: 0.6527, AVG_RECALL: 0.5595, AVG_F1: 0.5726\n",
            "Trying parameters: emb_dim=64, dropout=0.1, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0981, Jaccard: 0.4378,  PRAUC: 0.6783, AVG_PRC: 0.6623, AVG_RECALL: 0.5618, AVG_F1: 0.5776\n",
            "Trying parameters: emb_dim=64, dropout=0.1, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1016, Jaccard: 0.4432,  PRAUC: 0.6772, AVG_PRC: 0.6343, AVG_RECALL: 0.5912, AVG_F1: 0.5826\n",
            "Trying parameters: emb_dim=64, dropout=0.2, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0872, Jaccard: 0.4302,  PRAUC: 0.6683, AVG_PRC: 0.6551, AVG_RECALL: 0.5573, AVG_F1: 0.5721\n",
            "Trying parameters: emb_dim=64, dropout=0.2, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0952, Jaccard: 0.4397,  PRAUC: 0.6789, AVG_PRC: 0.6559, AVG_RECALL: 0.5693, AVG_F1: 0.5792\n",
            "Trying parameters: emb_dim=64, dropout=0.2, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1021, Jaccard: 0.4477,  PRAUC: 0.6797, AVG_PRC: 0.6324, AVG_RECALL: 0.6030, AVG_F1: 0.5876\n",
            "Trying parameters: emb_dim=64, dropout=0.3, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0876, Jaccard: 0.4312,  PRAUC: 0.6634, AVG_PRC: 0.6463, AVG_RECALL: 0.5629, AVG_F1: 0.5726\n",
            "Trying parameters: emb_dim=64, dropout=0.3, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0952, Jaccard: 0.4363,  PRAUC: 0.6797, AVG_PRC: 0.6580, AVG_RECALL: 0.5629, AVG_F1: 0.5770\n",
            "Trying parameters: emb_dim=64, dropout=0.3, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1017, Jaccard: 0.4462,  PRAUC: 0.6808, AVG_PRC: 0.6339, AVG_RECALL: 0.6003, AVG_F1: 0.5873\n",
            "Trying parameters: emb_dim=64, dropout=0.4, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0863, Jaccard: 0.4304,  PRAUC: 0.6626, AVG_PRC: 0.6516, AVG_RECALL: 0.5575, AVG_F1: 0.5721\n",
            "Trying parameters: emb_dim=64, dropout=0.4, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0911, Jaccard: 0.4400,  PRAUC: 0.6783, AVG_PRC: 0.6579, AVG_RECALL: 0.5678, AVG_F1: 0.5794\n",
            "Trying parameters: emb_dim=64, dropout=0.4, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0987, Jaccard: 0.4478,  PRAUC: 0.6851, AVG_PRC: 0.6403, AVG_RECALL: 0.5957, AVG_F1: 0.5875\n",
            "Trying parameters: emb_dim=64, dropout=0.5, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0868, Jaccard: 0.4374,  PRAUC: 0.6676, AVG_PRC: 0.6525, AVG_RECALL: 0.5692, AVG_F1: 0.5787\n",
            "Trying parameters: emb_dim=64, dropout=0.5, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0932, Jaccard: 0.4380,  PRAUC: 0.6783, AVG_PRC: 0.6668, AVG_RECALL: 0.5588, AVG_F1: 0.5791\n",
            "Trying parameters: emb_dim=64, dropout=0.5, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1007, Jaccard: 0.4533,  PRAUC: 0.6863, AVG_PRC: 0.6400, AVG_RECALL: 0.6053, AVG_F1: 0.5932\n",
            "Trying parameters: emb_dim=128, dropout=0.1, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0923, Jaccard: 0.4364,  PRAUC: 0.6761, AVG_PRC: 0.6569, AVG_RECALL: 0.5624, AVG_F1: 0.5754\n",
            "Trying parameters: emb_dim=128, dropout=0.1, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1017, Jaccard: 0.4388,  PRAUC: 0.6805, AVG_PRC: 0.6426, AVG_RECALL: 0.5818, AVG_F1: 0.5776\n",
            "Trying parameters: emb_dim=128, dropout=0.1, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1003, Jaccard: 0.4428,  PRAUC: 0.6799, AVG_PRC: 0.6364, AVG_RECALL: 0.5919, AVG_F1: 0.5835\n",
            "Trying parameters: emb_dim=128, dropout=0.2, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0957, Jaccard: 0.4420,  PRAUC: 0.6781, AVG_PRC: 0.6584, AVG_RECALL: 0.5704, AVG_F1: 0.5813\n",
            "Trying parameters: emb_dim=128, dropout=0.2, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1049, Jaccard: 0.4410,  PRAUC: 0.6809, AVG_PRC: 0.6453, AVG_RECALL: 0.5804, AVG_F1: 0.5818\n",
            "Trying parameters: emb_dim=128, dropout=0.2, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1041, Jaccard: 0.4471,  PRAUC: 0.6833, AVG_PRC: 0.6311, AVG_RECALL: 0.6051, AVG_F1: 0.5881\n",
            "Trying parameters: emb_dim=128, dropout=0.3, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0931, Jaccard: 0.4377,  PRAUC: 0.6785, AVG_PRC: 0.6645, AVG_RECALL: 0.5582, AVG_F1: 0.5779\n",
            "Trying parameters: emb_dim=128, dropout=0.3, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1033, Jaccard: 0.4431,  PRAUC: 0.6846, AVG_PRC: 0.6476, AVG_RECALL: 0.5827, AVG_F1: 0.5836\n",
            "Trying parameters: emb_dim=128, dropout=0.3, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0987, Jaccard: 0.4484,  PRAUC: 0.6859, AVG_PRC: 0.6287, AVG_RECALL: 0.6115, AVG_F1: 0.5895\n",
            "Trying parameters: emb_dim=128, dropout=0.4, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0889, Jaccard: 0.4391,  PRAUC: 0.6773, AVG_PRC: 0.6626, AVG_RECALL: 0.5644, AVG_F1: 0.5798\n",
            "Trying parameters: emb_dim=128, dropout=0.4, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1015, Jaccard: 0.4455,  PRAUC: 0.6849, AVG_PRC: 0.6571, AVG_RECALL: 0.5790, AVG_F1: 0.5858\n",
            "Trying parameters: emb_dim=128, dropout=0.4, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0962, Jaccard: 0.4592,  PRAUC: 0.6959, AVG_PRC: 0.6272, AVG_RECALL: 0.6308, AVG_F1: 0.6001\n",
            "Trying parameters: emb_dim=128, dropout=0.5, LR=0.0001, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0922, Jaccard: 0.4389,  PRAUC: 0.6779, AVG_PRC: 0.6610, AVG_RECALL: 0.5650, AVG_F1: 0.5803\n",
            "Trying parameters: emb_dim=128, dropout=0.5, LR=0.0002, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1033, Jaccard: 0.4429,  PRAUC: 0.6860, AVG_PRC: 0.6537, AVG_RECALL: 0.5743, AVG_F1: 0.5828\n",
            "Trying parameters: emb_dim=128, dropout=0.5, LR=0.0005, EPOCH=10\n",
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.0979, Jaccard: 0.4547,  PRAUC: 0.6913, AVG_PRC: 0.6343, AVG_RECALL: 0.6153, AVG_F1: 0.5948\n",
            "Best score:  0.6000705331677506\n",
            "Best parameters:  {'emb_dim': 128, 'dropout': 0.4, 'LR': 0.0005, 'EPOCH': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best parameters:  {'emb_dim': 128, 'dropout': 0.4, 'LR': 0.0005, 'EPOCH': 10}"
      ],
      "metadata": {
        "id": "95UXp4bwfPTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    # evaluate\n",
        "    print('')\n",
        "    model.eval()\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    case_study = defaultdict(dict)\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt = []\n",
        "        y_pred = []\n",
        "        y_pred_prob = []\n",
        "        y_pred_label = []\n",
        "        for adm_idx, adm in enumerate(input):\n",
        "\n",
        "            target_output1 = model(input[:adm_idx+1])\n",
        "\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output1)\n",
        "            y_pred_tmp = target_output1.copy()\n",
        "            y_pred_tmp[y_pred_tmp>=0.5] = 1\n",
        "            y_pred_tmp[y_pred_tmp<0.5] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "            y_pred_label_tmp = np.where(y_pred_tmp == 1)[0]\n",
        "            y_pred_label.append(sorted(y_pred_label_tmp))\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob))\n",
        "        case_study[adm_ja] = {'ja': adm_ja, 'patient': input, 'y_label': y_pred_label}\n",
        "\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record)\n",
        "\n",
        "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "    ))\n",
        "    #dill.dump(obj=smm_record, file=open('content/gcnrecords.pkl', 'wb'))\n",
        "    #dill.dump(case_study, open(os.path.join('saved', model_name, 'case_study.pkl'), 'wb'))\n",
        "\n",
        "    # print('avg med', med_cnt / visit_cnt)\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)"
      ],
      "metadata": {
        "id": "WE-kiNntht8m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import dill\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "from models_initialize import GCN\n",
        "from metrics import llprint, multi_label_metric, ddi_rate_score, get_n_params\n",
        "\n",
        "torch.manual_seed(1203)\n",
        "np.random.seed(1203)\n",
        "\n",
        "model_name = 'GCN'\n",
        "resume_name = ''\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--eval', action='store_true', default=False, help=\"eval mode\")\n",
        "parser.add_argument('--model_name', type=str, default=model_name, help=\"model name\")\n",
        "parser.add_argument('--resume_path', type=str, default=resume_name, help='resume path')\n",
        "parser.add_argument('--ddi', action='store_true', default=False, help=\"using ddi\")\n",
        "\n",
        "args, unknown = parser.parse_known_args()  # Replace here\n",
        "model_name = args.model_name\n",
        "resume_name = args.resume_path\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = '/content/patient_records_final.pkl'\n",
        "    voc_path = '/content/vocabulary_final.pkl'\n",
        "\n",
        "    ehr_adj_path = '/content/ehr_adjacency_matrix_final.pkl'\n",
        "    ddi_adj_path = '/content/ddi_adjacency_matrix_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n",
        "    ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "\n",
        "    #Optimal hyperparameters\n",
        "    emb_dim = 128\n",
        "    dropout = 0.4\n",
        "    lr = 0.0005\n",
        "    epoch = 10\n",
        "    Neg_Loss = args.ddi\n",
        "    DDI_IN_MEM = args.ddi\n",
        "    best_score = 0\n",
        "    best_params = {}\n",
        "\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    # Create a new instance of the model with the current set of parameters\n",
        "    model = GCN(voc_size, ehr_adj, ddi_adj, emb_dim=emb_dim, dropout=dropout, device=torch.device('cpu:0'), ddi_in_memory=DDI_IN_MEM)\n",
        "    #model.to(device)\n",
        "\n",
        "    # Train the model and get its performance on the validation set\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for current_epoch in range(epoch):\n",
        "        loss_record1 = []\n",
        "        prediction_loss_cnt = 0\n",
        "        neg_loss_cnt = 0\n",
        "        for step, input in enumerate(data_train):\n",
        "          #input = input.to(device)\n",
        "          for idx, adm in enumerate(input):\n",
        "              seq_input = input[:idx+1]\n",
        "              #seq_input = torch.tensor(seq_input).to(device)\n",
        "              loss1_target = np.zeros((1, voc_size[2]))\n",
        "              loss1_target[:, adm[2]] = 1\n",
        "              loss3_target = np.full((1, voc_size[2]), -1)\n",
        "              for idx, item in enumerate(adm[2]):\n",
        "                  loss3_target[0][idx] = item\n",
        "\n",
        "              target_output1, batch_neg_loss = model(seq_input)\n",
        "\n",
        "              loss1 = F.binary_cross_entropy_with_logits(target_output1, torch.FloatTensor(loss1_target).to(device))\n",
        "              loss3 = F.multilabel_margin_loss(F.sigmoid(target_output1), torch.LongTensor(loss3_target).to(device))\n",
        "              if Neg_Loss:\n",
        "                  target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
        "                  target_output1[target_output1 >= 0.5] = 1\n",
        "                  target_output1[target_output1 < 0.5] = 0\n",
        "                  y_label = np.where(target_output1 == 1)[0]\n",
        "                  current_ddi_rate = ddi_rate_score([[y_label]])\n",
        "                  if current_ddi_rate <= TARGET_DDI:\n",
        "                      loss = 0.9 * loss1 + 0.01 * loss3\n",
        "                      prediction_loss_cnt += 1\n",
        "                  else:\n",
        "                      rnd = np.exp((TARGET_DDI - current_ddi_rate)/T)\n",
        "                      if np.random.rand(1) < rnd:\n",
        "                          loss = batch_neg_loss\n",
        "                          neg_loss_cnt += 1\n",
        "                      else:\n",
        "                          loss = 0.9 * loss1 + 0.01 * loss3\n",
        "                          prediction_loss_cnt += 1\n",
        "              else:\n",
        "                  loss = 0.9 * loss1 + 0.01 * loss3\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward(retain_graph=True)\n",
        "              optimizer.step()\n",
        "\n",
        "              loss_record1.append(loss.item())\n",
        "\n",
        "    llprint('\\rTrain--Epoch: %d, Step: %d/%d, L_p cnt: %d, L_neg cnt: %d' % (epoch, step, len(data_train), prediction_loss_cnt, neg_loss_cnt))\n",
        "\n",
        "    # Evaluate the model\n",
        "    ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "    llprint('\\EVAL--Epoch: %d, Step: %d/%d, L_p cnt: %d, L_neg cnt: %d' % (epoch, step, len(data_train), prediction_loss_cnt, neg_loss_cnt))\n",
        "\n",
        "    ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_test, voc_size, 0)\n",
        "    llprint('\\TEST--Epoch: %d, Step: %d/%d, L_p cnt: %d, L_neg cnt: %d' % (epoch, step, len(data_train), prediction_loss_cnt, neg_loss_cnt))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fkX9p3_fOze",
        "outputId": "c0006528-6fc0-4066-b7cc-98c5a7f0fdcd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 10, Step: 769/770\tDDI Rate: 0.1002, Jaccard: 0.4586,  PRAUC: 0.6906, AVG_PRC: 0.6347, AVG_RECALL: 0.6208, AVG_F1: 0.5988\n",
            "\\EVAL--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0\n",
            "Eval--Epoch: 0, Step: 769/770\tDDI Rate: 0.0993, Jaccard: 0.4550,  PRAUC: 0.6943, AVG_PRC: 0.6525, AVG_RECALL: 0.5977, AVG_F1: 0.5982\n",
            "\\TEST--Epoch: 10, Step: 3077/3078, L_p cnt: 0, L_neg cnt: 0"
          ]
        }
      ]
    }
  ]
}