{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install dill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLzlyJIXLbGm",
        "outputId": "80537ddc-9f87-4540-9738-952802a5b961"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m112.6/115.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ndzk6T-wLJwz"
      },
      "outputs": [],
      "source": [
        "import dill\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from metrics import multi_label_metric\n",
        "\n",
        "data_path = 'patient_records_final.pkl'\n",
        "voc_path = 'vocabulary_final.pkl'\n",
        "\n",
        "ddi_adj_path = 'ddi_adjacency_matrix_final.pkl'\n",
        "\n",
        "data = dill.load(open(data_path, 'rb'))\n",
        "voc = dill.load(open(voc_path, 'rb'))\n",
        "diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "med_voc_size = len(med_voc.index_to_word)\n",
        "diag_voc_size = len(diag_voc.index_to_word)\n",
        "pro_voc_size = len(pro_voc.index_to_word)\n",
        "ddi_A = dill.load(open(ddi_adj_path, 'rb'))\n",
        "split_point = int(len(data) * 2 / 3)\n",
        "data_train = data[:split_point]\n",
        "eval_len = int(len(data[split_point:]) / 2)\n",
        "data_test = data[split_point:split_point + eval_len]\n",
        "data_eval = data[split_point+eval_len:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Near"
      ],
      "metadata": {
        "id": "qAdBVWqKMd-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    gt = []\n",
        "    pred = []\n",
        "    for patient in data_test:\n",
        "        if len(patient) == 1:\n",
        "            continue\n",
        "        for adm_idx, adm in enumerate(patient):\n",
        "            if adm_idx < len(patient)-1:\n",
        "                gt.append(patient[adm_idx+1][2])\n",
        "                pred.append(adm[2])\n",
        "    med_voc_size = len(med_voc.index_to_word)\n",
        "    y_gt = np.zeros((len(gt), med_voc_size))\n",
        "    y_pred = np.zeros((len(gt), med_voc_size))\n",
        "    for idx, item in enumerate(gt):\n",
        "        y_gt[idx, item] = 1\n",
        "    for idx, item in enumerate(pred):\n",
        "        y_pred[idx, item] = 1\n",
        "\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(y_gt, y_pred, y_pred)\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_A = dill.load(open(ddi_adj_path, 'rb'))\n",
        "    all_cnt = 0\n",
        "    dd_cnt = 0\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for adm in y_pred:\n",
        "        med_code_set = np.where(adm == 1)[0]\n",
        "        visit_cnt += 1\n",
        "        med_cnt += len(med_code_set)\n",
        "        for i, med_i in enumerate(med_code_set):\n",
        "            for j, med_j in enumerate(med_code_set):\n",
        "                if j <= i:\n",
        "                    continue\n",
        "                all_cnt += 1\n",
        "                if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                    dd_cnt += 1\n",
        "    ddi_rate = dd_cnt / all_cnt\n",
        "    print('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1\n",
        "    ))\n",
        "    print('avg med', med_cnt/ visit_cnt)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czftcmUPMKXA",
        "outputId": "2ccdfc36-d105-4a26-cec9-ddab8fb2aa4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDDI Rate: 0.0989, Jaccard: 0.3524,  PRAUC: 0.3374, AVG_PRC: 0.5241, AVG_RECALL: 0.5276, AVG_F1: 0.5041\n",
            "\n",
            "avg med 13.542857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "9GBpUHoUM1GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import jaccard_score\n",
        "import os\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from metrics import multi_label_metric\n",
        "\n",
        "np.random.seed(1203)\n",
        "model_name = 'LR'\n",
        "\n",
        "if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "    os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "def create_dataset(data, diag_voc, pro_voc, med_voc):\n",
        "    i1_len = diag_voc_size\n",
        "    i2_len = pro_voc_size\n",
        "    output_len = med_voc_size\n",
        "    input_len = i1_len + i2_len\n",
        "    X = []\n",
        "    y = []\n",
        "    for patient in data:\n",
        "        for visit in patient:\n",
        "            i1 = visit[0]\n",
        "            i2 = visit[1]\n",
        "            o = visit[2]\n",
        "\n",
        "            multi_hot_input = np.zeros(input_len)\n",
        "            multi_hot_input[i1] = 1\n",
        "            multi_hot_input[np.array(i2) + i1_len] = 1\n",
        "\n",
        "            multi_hot_output = np.zeros(output_len)\n",
        "            multi_hot_output[o] = 1\n",
        "\n",
        "            X.append(multi_hot_input)\n",
        "            y.append(multi_hot_output)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "def main():\n",
        "    grid_search = False\n",
        "    data_path = 'patient_records_final.pkl'\n",
        "    voc_path = 'vocabulary_final.pkl'\n",
        "\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "\n",
        "    train_X, train_y = create_dataset(data_train, diag_voc, pro_voc, med_voc)\n",
        "    test_X, test_y = create_dataset(data_test, diag_voc, pro_voc, med_voc)\n",
        "    eval_X, eval_y = create_dataset(data_eval, diag_voc, pro_voc, med_voc)\n",
        "\n",
        "    if grid_search:\n",
        "        params = {\n",
        "            'estimator__penalty': ['l2'],\n",
        "            'estimator__C': np.linspace(0.00002, 1, 10)\n",
        "        }\n",
        "\n",
        "        model = LogisticRegression()\n",
        "        classifier = OneVsRestClassifier(model)\n",
        "        lr_gs = GridSearchCV(classifier, params, verbose=1).fit(train_X, train_y)\n",
        "\n",
        "        print(\"Best Params\", lr_gs.best_params_)\n",
        "        print(\"Best Score\", lr_gs.best_score_)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    # sample_X, sample_y = create_dataset(sample_data, diag_voc, pro_voc, med_voc)\n",
        "\n",
        "    model = LogisticRegression(C=0.90909)\n",
        "    classifier = OneVsRestClassifier(model)\n",
        "    classifier.fit(train_X, train_y)\n",
        "\n",
        "    y_eval_pred = classifier.predict(eval_X)\n",
        "    y_eval_prob = classifier.predict_proba(eval_X)\n",
        "\n",
        "    model = LogisticRegression(C=0.90909)\n",
        "    classifier = OneVsRestClassifier(model)\n",
        "    classifier.fit(train_X, train_y)\n",
        "\n",
        "    y_pred = classifier.predict(test_X)\n",
        "    y_prob = classifier.predict_proba(test_X)\n",
        "\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(test_y, y_pred, y_prob)\n",
        "    eval_ja, eval_prauc, eval_avg_p, eval_avg_r, eval_avg_f1 = multi_label_metric(eval_y, y_eval_pred, y_eval_prob)\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_A = dill.load(open(ddi_adj_path, 'rb'))\n",
        "    all_cnt = 0\n",
        "    dd_cnt = 0\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    all_eval_cnt = 0\n",
        "    dd_eval_cnt = 0\n",
        "    med_eval_cnt = 0\n",
        "    visit_eval_cnt = 0\n",
        "    for adm in y_pred:\n",
        "        med_code_set = np.where(adm==1)[0]\n",
        "        visit_cnt += 1\n",
        "        med_cnt += len(med_code_set)\n",
        "        for i, med_i in enumerate(med_code_set):\n",
        "            for j, med_j in enumerate(med_code_set):\n",
        "                if j <= i:\n",
        "                    continue\n",
        "                all_cnt += 1\n",
        "                if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                    dd_cnt += 1\n",
        "    for adm in y_eval_pred:\n",
        "      med_code_set = np.where(adm==1)[0]\n",
        "      visit_eval_cnt += 1\n",
        "      med_eval_cnt += len(med_code_set)\n",
        "      for i, med_i in enumerate(med_code_set):\n",
        "          for j, med_j in enumerate(med_code_set):\n",
        "              if j <= i:\n",
        "                  continue\n",
        "              all_eval_cnt += 1\n",
        "              if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
        "                  dd_eval_cnt += 1\n",
        "    ddi_rate = dd_cnt / all_cnt\n",
        "    ddi_eval_rate = dd_eval_cnt/ all_eval_cnt\n",
        "    print('\\tTEST: DDI Rate: %.4f, Jaccard: %.4f, PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1\n",
        "    ))\n",
        "    print('\\tECAL: DDI Rate: %.4f, Jaccard: %.4f, PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_eval_rate, eval_ja, eval_prauc, eval_avg_p, eval_avg_r, eval_avg_f1\n",
        "    ))\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    for i in range(30):\n",
        "        history['jaccard'].append(ja)\n",
        "        history['ddi_rate'].append(ddi_rate)\n",
        "        history['avg_p'].append(avg_p)\n",
        "        history['avg_r'].append(avg_r)\n",
        "        history['avg_f1'].append(avg_f1)\n",
        "        history['prauc'].append(prauc)\n",
        "\n",
        "    dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
        "\n",
        "    print('avg med', med_cnt / visit_cnt)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90MrrouTMhkp",
        "outputId": "f82f8158-79e5-48ed-fc24-8b34741f5538"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTEST: DDI Rate: 0.0897, Jaccard: 0.4209, PRAUC: 0.6852, AVG_PRC: 0.6741, AVG_RECALL: 0.5234, AVG_F1: 0.5651\n",
            "\n",
            "\tECAL: DDI Rate: 0.0912, Jaccard: 0.4176, PRAUC: 0.6728, AVG_PRC: 0.6515, AVG_RECALL: 0.5300, AVG_F1: 0.5586\n",
            "\n",
            "avg med 8.6935960591133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LEAP"
      ],
      "metadata": {
        "id": "UwHWs64iX5MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from sklearn.metrics import jaccard_similarity_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "import numpy as np\n",
        "import dill\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from models_initialize import Leap\n",
        "from metrics import llprint, sequence_metric, sequence_output_process, ddi_rate_score, get_n_params\n",
        "\n",
        "torch.manual_seed(1203)\n",
        "\n",
        "model_name = 'Leap'\n",
        "resume_name = ''\n",
        "\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    # evaluate\n",
        "    print('')\n",
        "    model.eval()\n",
        "\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    records = []\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt = []\n",
        "        y_pred = []\n",
        "        y_pred_prob = []\n",
        "        y_pred_label = []\n",
        "        for adm in input:\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            output_logits = model(adm)\n",
        "            output_logits = output_logits.detach().cpu().numpy()\n",
        "\n",
        "            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n",
        "\n",
        "            y_pred_label.append(sorted(sorted_predict))\n",
        "            y_pred_prob.append(np.mean(output_logits[:, :-2], axis=0))\n",
        "\n",
        "            y_pred_tmp = np.zeros(voc_size[2])\n",
        "            y_pred_tmp[out_list] = 1\n",
        "            y_pred.append(y_pred_tmp)\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(sorted_predict)\n",
        "        records.append(y_pred_label)\n",
        "\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob), np.array(y_pred_label))\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(records)\n",
        "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "    ))\n",
        "    print('avg med', med_cnt / visit_cnt)\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = 'patient_records_final.pkl'\n",
        "    voc_path = 'vocabulary_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    EPOCH = 10\n",
        "    LR = 0.0002\n",
        "    TEST = False\n",
        "    END_TOKEN = voc_size[2] + 1\n",
        "\n",
        "    model = Leap(voc_size, device=device)\n",
        "    if TEST:\n",
        "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
        "        # pass\n",
        "\n",
        "    model.to(device=device)\n",
        "    print('parameters', get_n_params(model))\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    if TEST:\n",
        "        eval(model, data_test, voc_size, 0)\n",
        "    else:\n",
        "        history = defaultdict(list)\n",
        "        for epoch in range(EPOCH):\n",
        "            loss_record = []\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            for step, input in enumerate(data_train):\n",
        "                for adm in input:\n",
        "                    loss_target = adm[2] + [END_TOKEN]\n",
        "                    output_logits = model(adm)\n",
        "                    loss = F.cross_entropy(output_logits, torch.LongTensor(loss_target).to(device))\n",
        "\n",
        "                    loss_record.append(loss.item())\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    optimizer.step()\n",
        "\n",
        "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
        "\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "            history['ja'].append(ja)\n",
        "            history['ddi_rate'].append(ddi_rate)\n",
        "            history['avg_p'].append(avg_p)\n",
        "            history['avg_r'].append(avg_r)\n",
        "            history['avg_f1'].append(avg_f1)\n",
        "            history['prauc'].append(prauc)\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = (end_time - start_time) / 60\n",
        "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
        "                                                                                                np.mean(loss_record),\n",
        "                                                                                                elapsed_time,\n",
        "                                                                                                elapsed_time * (\n",
        "                                                                                                            EPOCH - epoch - 1)/60))\n",
        "\n",
        "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
        "            print('')\n",
        "\n",
        "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
        "        # test\n",
        "        torch.save(model.state_dict(), open(\n",
        "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3qzQsdQQtme",
        "outputId": "9a5754cb-74e4-4b07-9420-f62f1ebee487"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters 624651\n",
            "Train--Epoch: 0, Step: 3077/3078\n",
            "Eval--Epoch: 0, Step: 769/770\tDDI Rate: 0.0844, Jaccard: 0.3823,  PRAUC: 0.6262, AVG_PRC: 0.5605, AVG_RECALL: 0.5475, AVG_F1: 0.5237\n",
            "avg med 11.109657947686117\n",
            "\tEpoch: 0, Loss1: 3.1095, One Epoch Time: 2.22m, Appro Left Time: 0.33h\n",
            "\n",
            "Train--Epoch: 1, Step: 3077/3078\n",
            "Eval--Epoch: 1, Step: 769/770\tDDI Rate: 0.0711, Jaccard: 0.3837,  PRAUC: 0.6022, AVG_PRC: 0.5446, AVG_RECALL: 0.5600, AVG_F1: 0.5220\n",
            "avg med 11.753521126760564\n",
            "\tEpoch: 1, Loss1: 2.7191, One Epoch Time: 2.29m, Appro Left Time: 0.31h\n",
            "\n",
            "Train--Epoch: 2, Step: 3077/3078\n",
            "Eval--Epoch: 2, Step: 769/770\tDDI Rate: 0.0737, Jaccard: 0.3790,  PRAUC: 0.5846, AVG_PRC: 0.5312, AVG_RECALL: 0.5651, AVG_F1: 0.5180\n",
            "avg med 12.17102615694165\n",
            "\tEpoch: 2, Loss1: 2.6016, One Epoch Time: 2.28m, Appro Left Time: 0.27h\n",
            "\n",
            "Train--Epoch: 3, Step: 3077/3078\n",
            "Eval--Epoch: 3, Step: 769/770\tDDI Rate: 0.0734, Jaccard: 0.3807,  PRAUC: 0.5709, AVG_PRC: 0.5243, AVG_RECALL: 0.5779, AVG_F1: 0.5198\n",
            "avg med 12.758551307847082\n",
            "\tEpoch: 3, Loss1: 2.5224, One Epoch Time: 2.25m, Appro Left Time: 0.23h\n",
            "\n",
            "Train--Epoch: 4, Step: 3077/3078\n",
            "Eval--Epoch: 4, Step: 769/770\tDDI Rate: 0.0798, Jaccard: 0.3831,  PRAUC: 0.5716, AVG_PRC: 0.5155, AVG_RECALL: 0.5994, AVG_F1: 0.5224\n",
            "avg med 13.85513078470825\n",
            "\tEpoch: 4, Loss1: 2.4659, One Epoch Time: 2.26m, Appro Left Time: 0.19h\n",
            "\n",
            "Train--Epoch: 5, Step: 3077/3078\n",
            "Eval--Epoch: 5, Step: 769/770\tDDI Rate: 0.0754, Jaccard: 0.3909,  PRAUC: 0.5686, AVG_PRC: 0.5107, AVG_RECALL: 0.6185, AVG_F1: 0.5294\n",
            "avg med 14.489939637826962\n",
            "\tEpoch: 5, Loss1: 2.4197, One Epoch Time: 2.26m, Appro Left Time: 0.15h\n",
            "\n",
            "Train--Epoch: 6, Step: 3077/3078\n",
            "Eval--Epoch: 6, Step: 769/770\tDDI Rate: 0.0793, Jaccard: 0.4012,  PRAUC: 0.5692, AVG_PRC: 0.5223, AVG_RECALL: 0.6353, AVG_F1: 0.5405\n",
            "avg med 14.653923541247485\n",
            "\tEpoch: 6, Loss1: 2.3784, One Epoch Time: 2.27m, Appro Left Time: 0.11h\n",
            "\n",
            "Train--Epoch: 7, Step: 3077/3078\n",
            "Eval--Epoch: 7, Step: 769/770\tDDI Rate: 0.0809, Jaccard: 0.3975,  PRAUC: 0.5744, AVG_PRC: 0.5244, AVG_RECALL: 0.6260, AVG_F1: 0.5376\n",
            "avg med 14.507042253521126\n",
            "\tEpoch: 7, Loss1: 2.3479, One Epoch Time: 2.28m, Appro Left Time: 0.08h\n",
            "\n",
            "Train--Epoch: 8, Step: 3077/3078\n",
            "Eval--Epoch: 8, Step: 769/770\tDDI Rate: 0.0783, Jaccard: 0.3956,  PRAUC: 0.5707, AVG_PRC: 0.5238, AVG_RECALL: 0.6195, AVG_F1: 0.5348\n",
            "avg med 14.345070422535212\n",
            "\tEpoch: 8, Loss1: 2.3144, One Epoch Time: 2.25m, Appro Left Time: 0.04h\n",
            "\n",
            "Train--Epoch: 9, Step: 3077/3078\n",
            "Eval--Epoch: 9, Step: 769/770\tDDI Rate: 0.0787, Jaccard: 0.3998,  PRAUC: 0.5774, AVG_PRC: 0.5316, AVG_RECALL: 0.6202, AVG_F1: 0.5389\n",
            "avg med 14.194164989939638\n",
            "\tEpoch: 9, Loss1: 2.2860, One Epoch Time: 2.27m, Appro Left Time: 0.00h\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1203)\n",
        "\n",
        "model_name = 'Leap'\n",
        "resume_name = ''\n",
        "\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    # evaluate\n",
        "    print('')\n",
        "    model.eval()\n",
        "\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    records = []\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for step, input in enumerate(data_eval):\n",
        "        y_gt = []\n",
        "        y_pred = []\n",
        "        y_pred_prob = []\n",
        "        y_pred_label = []\n",
        "        for adm in input:\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[adm[2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            output_logits = model(adm)\n",
        "            output_logits = output_logits.detach().cpu().numpy()\n",
        "\n",
        "            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n",
        "\n",
        "            y_pred_label.append(sorted(sorted_predict))\n",
        "            y_pred_prob.append(np.mean(output_logits[:, :-2], axis=0))\n",
        "\n",
        "            y_pred_tmp = np.zeros(voc_size[2])\n",
        "            y_pred_tmp[out_list] = 1\n",
        "            y_pred.append(y_pred_tmp)\n",
        "            visit_cnt += 1\n",
        "            med_cnt += len(sorted_predict)\n",
        "        records.append(y_pred_label)\n",
        "\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob), np.array(y_pred_label))\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
        "\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(records)\n",
        "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "    ))\n",
        "    print('avg med', med_cnt / visit_cnt)\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = 'patient_records_final.pkl'\n",
        "    voc_path = 'vocabulary_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    EPOCH = 10\n",
        "    LR = 0.0002\n",
        "    TEST = True\n",
        "    END_TOKEN = voc_size[2] + 1\n",
        "\n",
        "    model = Leap(voc_size, device=device)\n",
        "    if TEST:\n",
        "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, 'final.model'), 'rb')))\n",
        "        # pass\n",
        "\n",
        "    model.to(device=device)\n",
        "    print('parameters', get_n_params(model))\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    if TEST:\n",
        "        eval(model, data_test, voc_size, 0)\n",
        "    else:\n",
        "        history = defaultdict(list)\n",
        "        for epoch in range(EPOCH):\n",
        "            loss_record = []\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            for step, input in enumerate(data_train):\n",
        "                for adm in input:\n",
        "                    loss_target = adm[2] + [END_TOKEN]\n",
        "                    output_logits = model(adm)\n",
        "                    loss = F.cross_entropy(output_logits, torch.LongTensor(loss_target).to(device))\n",
        "\n",
        "                    loss_record.append(loss.item())\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    optimizer.step()\n",
        "\n",
        "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
        "\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "            history['ja'].append(ja)\n",
        "            history['ddi_rate'].append(ddi_rate)\n",
        "            history['avg_p'].append(avg_p)\n",
        "            history['avg_r'].append(avg_r)\n",
        "            history['avg_f1'].append(avg_f1)\n",
        "            history['prauc'].append(prauc)\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = (end_time - start_time) / 60\n",
        "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
        "                                                                                                np.mean(loss_record),\n",
        "                                                                                                elapsed_time,\n",
        "                                                                                                elapsed_time * (\n",
        "                                                                                                            EPOCH - epoch - 1)/60))\n",
        "\n",
        "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
        "            print('')\n",
        "\n",
        "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
        "        # test\n",
        "        torch.save(model.state_dict(), open(\n",
        "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Di-Lo9-ikr",
        "outputId": "7d1a12cc-7b2b-4077-c795-34a425798c02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters 624651\n",
            "\n",
            "Eval--Epoch: 0, Step: 769/770\tDDI Rate: 0.0812, Jaccard: 0.4013,  PRAUC: 0.5840, AVG_PRC: 0.5459, AVG_RECALL: 0.6082, AVG_F1: 0.5432\n",
            "avg med 14.057142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RETAIN"
      ],
      "metadata": {
        "id": "9-woOcShZgAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
        "import numpy as np\n",
        "import dill\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from models_initialize import Retain\n",
        "from metrics import llprint, multi_label_metric, ddi_rate_score, get_n_params\n",
        "\n",
        "torch.manual_seed(1203)\n",
        "model_name = 'Retain'\n",
        "resume_name = ''\n",
        "\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    # evaluate\n",
        "    print('')\n",
        "    model.eval()\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    case_study = defaultdict(dict)\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for step, input in enumerate(data_eval):\n",
        "        if len(input) < 2: # visit > 2\n",
        "            continue\n",
        "        y_gt = []\n",
        "        y_pred = []\n",
        "        y_pred_prob = []\n",
        "        y_pred_label = []\n",
        "        for i in range(1, len(input)):\n",
        "\n",
        "            y_pred_label_tmp = []\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[input[i][2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            target_output1 = model(input[:i])\n",
        "\n",
        "            target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output1)\n",
        "            y_pred_tmp = target_output1.copy()\n",
        "            y_pred_tmp[y_pred_tmp >= 0.3] = 1\n",
        "            y_pred_tmp[y_pred_tmp < 0.3] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "            for idx, value in enumerate(y_pred_tmp):\n",
        "                if value == 1:\n",
        "                    y_pred_label_tmp.append(idx)\n",
        "            y_pred_label.append(y_pred_label_tmp)\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "            visit_cnt += 1\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred),\n",
        "                                                                                   np.array(y_pred_prob))\n",
        "        case_study[adm_ja] = {'ja': adm_ja, 'patient':input, 'y_label':y_pred_label}\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
        "\n",
        "    dill.dump(case_study, open(os.path.join('saved', model_name, 'case_study.pkl'), 'wb'))\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record)\n",
        "\n",
        "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "    ))\n",
        "    print('avg med', med_cnt / visit_cnt)\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = 'patient_records_final.pkl'\n",
        "    voc_path = 'vocabulary_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    EPOCH = 10\n",
        "    LR = 0.0002\n",
        "    TEST = False\n",
        "\n",
        "    model = Retain(voc_size, device=device)\n",
        "    if TEST:\n",
        "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
        "\n",
        "    model.to(device=device)\n",
        "    print('parameters', get_n_params(model))\n",
        "\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    if TEST:\n",
        "        eval(model, data_test, voc_size, 0)\n",
        "    else:\n",
        "        history = defaultdict(list)\n",
        "        for epoch in range(EPOCH):\n",
        "            loss_record = []\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            for step, input in enumerate(data_train):\n",
        "                if len(input) < 2:\n",
        "                    continue\n",
        "\n",
        "                loss = 0\n",
        "                for i in range(1, len(input)):\n",
        "                    target = np.zeros((1, voc_size[2]))\n",
        "                    target[:, input[i][2]] = 1\n",
        "\n",
        "                    output_logits = model(input[:i])\n",
        "                    loss += F.binary_cross_entropy_with_logits(output_logits, torch.FloatTensor(target).to(device))\n",
        "                    loss_record.append(loss.item())\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
        "\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "            history['ja'].append(ja)\n",
        "            history['ddi_rate'].append(ddi_rate)\n",
        "            history['avg_p'].append(avg_p)\n",
        "            history['avg_r'].append(avg_r)\n",
        "            history['avg_f1'].append(avg_f1)\n",
        "            history['prauc'].append(prauc)\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = (end_time - start_time) / 60\n",
        "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
        "                                                                                                np.mean(loss_record),\n",
        "                                                                                                elapsed_time,\n",
        "                                                                                                elapsed_time * (\n",
        "                                                                                                            EPOCH - epoch - 1)/60))\n",
        "\n",
        "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
        "            print('')\n",
        "\n",
        "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
        "\n",
        "        # test\n",
        "        torch.save(model.state_dict(), open(\n",
        "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVi-SAXhZpMW",
        "outputId": "878fcba6-67dc-49ae-b17b-97875338afe3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters 363529\n",
            "Train--Epoch: 0, Step: 3053/3078\n",
            "Eval--Epoch: 0, Step: 769/770\tDDI Rate: 0.0561, Jaccard: 0.1020,  PRAUC: 0.1828, AVG_PRC: 0.1089, AVG_RECALL: 0.7151, AVG_F1: 0.1823\n",
            "avg med 77.85267857142857\n",
            "\tEpoch: 0, Loss1: 2.7789, One Epoch Time: 0.16m, Appro Left Time: 0.02h\n",
            "\n",
            "Train--Epoch: 1, Step: 3053/3078\n",
            "Eval--Epoch: 1, Step: 769/770\tDDI Rate: 0.0646, Jaccard: 0.1564,  PRAUC: 0.3113, AVG_PRC: 0.1843, AVG_RECALL: 0.6587, AVG_F1: 0.2635\n",
            "avg med 47.339285714285715\n",
            "\tEpoch: 1, Loss1: 1.6426, One Epoch Time: 0.23m, Appro Left Time: 0.03h\n",
            "\n",
            "Train--Epoch: 2, Step: 3053/3078\n",
            "Eval--Epoch: 2, Step: 769/770\tDDI Rate: 0.0719, Jaccard: 0.2056,  PRAUC: 0.3886, AVG_PRC: 0.2538, AVG_RECALL: 0.6603, AVG_F1: 0.3314\n",
            "avg med 35.95982142857143\n",
            "\tEpoch: 2, Loss1: 1.0956, One Epoch Time: 0.20m, Appro Left Time: 0.02h\n",
            "\n",
            "Train--Epoch: 3, Step: 3053/3078\n",
            "Eval--Epoch: 3, Step: 769/770\tDDI Rate: 0.0796, Jaccard: 0.2368,  PRAUC: 0.4244, AVG_PRC: 0.2930, AVG_RECALL: 0.6677, AVG_F1: 0.3716\n",
            "avg med 30.767857142857142\n",
            "\tEpoch: 3, Loss1: 0.9206, One Epoch Time: 0.16m, Appro Left Time: 0.02h\n",
            "\n",
            "Train--Epoch: 4, Step: 3053/3078\n",
            "Eval--Epoch: 4, Step: 769/770\tDDI Rate: 0.0855, Jaccard: 0.2551,  PRAUC: 0.4453, AVG_PRC: 0.3145, AVG_RECALL: 0.6768, AVG_F1: 0.3943\n",
            "avg med 28.410714285714285\n",
            "\tEpoch: 4, Loss1: 0.8493, One Epoch Time: 0.18m, Appro Left Time: 0.01h\n",
            "\n",
            "Train--Epoch: 5, Step: 3053/3078\n",
            "Eval--Epoch: 5, Step: 769/770\tDDI Rate: 0.0905, Jaccard: 0.2644,  PRAUC: 0.4629, AVG_PRC: 0.3237, AVG_RECALL: 0.6875, AVG_F1: 0.4061\n",
            "avg med 27.285714285714285\n",
            "\tEpoch: 5, Loss1: 0.7830, One Epoch Time: 0.15m, Appro Left Time: 0.01h\n",
            "\n",
            "Train--Epoch: 6, Step: 3053/3078\n",
            "Eval--Epoch: 6, Step: 769/770\tDDI Rate: 0.0942, Jaccard: 0.2726,  PRAUC: 0.4751, AVG_PRC: 0.3328, AVG_RECALL: 0.6905, AVG_F1: 0.4156\n",
            "avg med 26.575892857142858\n",
            "\tEpoch: 6, Loss1: 0.7297, One Epoch Time: 0.16m, Appro Left Time: 0.01h\n",
            "\n",
            "Train--Epoch: 7, Step: 3053/3078\n",
            "Eval--Epoch: 7, Step: 769/770\tDDI Rate: 0.0971, Jaccard: 0.2788,  PRAUC: 0.4871, AVG_PRC: 0.3396, AVG_RECALL: 0.6879, AVG_F1: 0.4226\n",
            "avg med 25.866071428571427\n",
            "\tEpoch: 7, Loss1: 0.6531, One Epoch Time: 0.14m, Appro Left Time: 0.00h\n",
            "\n",
            "Train--Epoch: 8, Step: 3053/3078\n",
            "Eval--Epoch: 8, Step: 769/770\tDDI Rate: 0.1008, Jaccard: 0.2876,  PRAUC: 0.4941, AVG_PRC: 0.3523, AVG_RECALL: 0.6855, AVG_F1: 0.4327\n",
            "avg med 25.022321428571427\n",
            "\tEpoch: 8, Loss1: 0.6221, One Epoch Time: 0.17m, Appro Left Time: 0.00h\n",
            "\n",
            "Train--Epoch: 9, Step: 3053/3078\n",
            "Eval--Epoch: 9, Step: 769/770\tDDI Rate: 0.1018, Jaccard: 0.2919,  PRAUC: 0.5038, AVG_PRC: 0.3573, AVG_RECALL: 0.6875, AVG_F1: 0.4376\n",
            "avg med 24.794642857142858\n",
            "\tEpoch: 9, Loss1: 0.6079, One Epoch Time: 0.15m, Appro Left Time: 0.00h\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from models_initialize import Retain\n",
        "from metrics import llprint, multi_label_metric, ddi_rate_score, get_n_params\n",
        "\n",
        "torch.manual_seed(1203)\n",
        "\n",
        "def eval(model, data_eval, voc_size, epoch):\n",
        "    # evaluate\n",
        "    print('')\n",
        "    model.eval()\n",
        "    smm_record = []\n",
        "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
        "    case_study = defaultdict(dict)\n",
        "    med_cnt = 0\n",
        "    visit_cnt = 0\n",
        "    for step, input in enumerate(data_eval):\n",
        "        if len(input) < 2: # visit > 2\n",
        "            continue\n",
        "        y_gt = []\n",
        "        y_pred = []\n",
        "        y_pred_prob = []\n",
        "        y_pred_label = []\n",
        "        for i in range(1, len(input)):\n",
        "\n",
        "            y_pred_label_tmp = []\n",
        "            y_gt_tmp = np.zeros(voc_size[2])\n",
        "            y_gt_tmp[input[i][2]] = 1\n",
        "            y_gt.append(y_gt_tmp)\n",
        "\n",
        "            target_output1 = model(input[:i])\n",
        "\n",
        "            target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
        "            y_pred_prob.append(target_output1)\n",
        "            y_pred_tmp = target_output1.copy()\n",
        "            y_pred_tmp[y_pred_tmp >= 0.3] = 1\n",
        "            y_pred_tmp[y_pred_tmp < 0.3] = 0\n",
        "            y_pred.append(y_pred_tmp)\n",
        "            for idx, value in enumerate(y_pred_tmp):\n",
        "                if value == 1:\n",
        "                    y_pred_label_tmp.append(idx)\n",
        "            y_pred_label.append(y_pred_label_tmp)\n",
        "            med_cnt += len(y_pred_label_tmp)\n",
        "            visit_cnt += 1\n",
        "\n",
        "        smm_record.append(y_pred_label)\n",
        "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred),\n",
        "                                                                                   np.array(y_pred_prob))\n",
        "        case_study[adm_ja] = {'ja': adm_ja, 'patient':input, 'y_label':y_pred_label}\n",
        "        ja.append(adm_ja)\n",
        "        prauc.append(adm_prauc)\n",
        "        avg_p.append(adm_avg_p)\n",
        "        avg_r.append(adm_avg_r)\n",
        "        avg_f1.append(adm_avg_f1)\n",
        "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
        "\n",
        "    dill.dump(case_study, open(os.path.join('saved', model_name, 'case_study.pkl'), 'wb'))\n",
        "    # ddi rate\n",
        "    ddi_rate = ddi_rate_score(smm_record)\n",
        "\n",
        "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
        "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "    ))\n",
        "    print('avg med', med_cnt / visit_cnt)\n",
        "\n",
        "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
        "        os.makedirs(os.path.join(\"saved\", model_name))\n",
        "\n",
        "    data_path = 'patient_records_final.pkl'\n",
        "    voc_path = 'vocabulary_final.pkl'\n",
        "    device = torch.device('cpu:0')\n",
        "\n",
        "    data = dill.load(open(data_path, 'rb'))\n",
        "    voc = dill.load(open(voc_path, 'rb'))\n",
        "    diag_voc, pro_voc, med_voc = voc['diagnosis_vocabulary'], voc['procedure_vocabulary'], voc['medicine_vocabulary']\n",
        "\n",
        "    split_point = int(len(data) * 2 / 3)\n",
        "    data_train = data[:split_point]\n",
        "    eval_len = int(len(data[split_point:]) / 2)\n",
        "    data_test = data[split_point:split_point + eval_len]\n",
        "    data_eval = data[split_point+eval_len:]\n",
        "    voc_size = (len(diag_voc.index_to_word), len(pro_voc.index_to_word), len(med_voc.index_to_word))\n",
        "\n",
        "    EPOCH = 10\n",
        "    LR = 0.0002\n",
        "    TEST = True\n",
        "\n",
        "    model = Retain(voc_size, device=device)\n",
        "    if TEST:\n",
        "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, 'final.model'), 'rb')))\n",
        "\n",
        "    model.to(device=device)\n",
        "    print('parameters', get_n_params(model))\n",
        "\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    if TEST:\n",
        "        eval(model, data_test, voc_size, 0)\n",
        "    else:\n",
        "        history = defaultdict(list)\n",
        "        for epoch in range(EPOCH):\n",
        "            loss_record = []\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            for step, input in enumerate(data_train):\n",
        "                if len(input) < 2:\n",
        "                    continue\n",
        "\n",
        "                loss = 0\n",
        "                for i in range(1, len(input)):\n",
        "                    target = np.zeros((1, voc_size[2]))\n",
        "                    target[:, input[i][2]] = 1\n",
        "\n",
        "                    output_logits = model(input[:i])\n",
        "                    loss += F.binary_cross_entropy_with_logits(output_logits, torch.FloatTensor(target).to(device))\n",
        "                    loss_record.append(loss.item())\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
        "\n",
        "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
        "            history['ja'].append(ja)\n",
        "            history['ddi_rate'].append(ddi_rate)\n",
        "            history['avg_p'].append(avg_p)\n",
        "            history['avg_r'].append(avg_r)\n",
        "            history['avg_f1'].append(avg_f1)\n",
        "            history['prauc'].append(prauc)\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = (end_time - start_time) / 60\n",
        "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
        "                                                                                                np.mean(loss_record),\n",
        "                                                                                                elapsed_time,\n",
        "                                                                                                elapsed_time * (\n",
        "                                                                                                            EPOCH - epoch - 1)/60))\n",
        "\n",
        "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
        "            print('')\n",
        "\n",
        "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
        "\n",
        "        # test\n",
        "        torch.save(model.state_dict(), open(\n",
        "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV_LjOKLJjNW",
        "outputId": "7e411d66-fdcb-4cdb-8c70-a21ae57e33a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters 363529\n",
            "\n",
            "Eval--Epoch: 0, Step: 769/770\tDDI Rate: 0.1051, Jaccard: 0.3221,  PRAUC: 0.5526, AVG_PRC: 0.3969, AVG_RECALL: 0.7011, AVG_F1: 0.4735\n",
            "avg med 22.551020408163264\n"
          ]
        }
      ]
    }
  ]
}